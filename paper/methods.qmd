## Corpus assembly

The US General Services Administration receives and publishes public comments on proposed rules using the website Regulations.gov.  Commenters may submit comments by online form, email, or physical mail, and electronic formats allow commenters to include a number of attachments.  Attachments are typically detailed comments or to include journal articles, reports, etc., as supporting evidence.  

All agency-published documents and public comments associated with a proposed rule are collected together in a docket, e.g., EPA-HQ-OA-2018-0259 for Strengthening Transparency, available at <https://www.regulations.gov/docket/EPA-HQ-OA-2018-0259>.  Regulations.gov provides an API (application programming interface) via http, meaning a simple script can be used to automatically retrieve ("scrape") the full contents of a given docket.  [A1] used the API to retrieve all publicly-available comments — including comment metadata, comment text, and any attachments — throughout 2020 and 2021, completing retrieving in June 2021.  

For Strengthening Transparency, Regulations.gov reports 993,464 "comments received" and 22,390 "comments posted to this docket" and publicly available.  *[TODO: email for clarification on relationship between these numbers]*  It's likely that this disparity is due, in part, to separately enumerating some mass comments that were submitted collectively.  For example, comment 11822 (<https://www.regulations.gov/comment/EPA-HQ-OA-2018-0259-11822>) includes 22,786 individual comments collected by Friends of the Earth (likely using an online form) and submitted as large PDFs, one individual comment per page.  

After scraping, our corpus included the text of 22,390 comments and attachments for 20,787 distinct comments.  *[try reruning scraper sometime]*  By number of attachments, the median document had no attachments and the maximum number of attachments was 17.  

## Sampling

Initial, undirected exploration of the corpus by [A1] indicated that (a) there was a qualitatively clear division between supporters and opponents of ST, (b) commenters offered substantive reasons for their position on ST, but (c) opposing comments were much, much more prevalent than supportive comments.  The first point meant that manual qualitative coding could be used to identify supporting and opposing comments; combined with the second point, this meant that the corpus was suitable for contrasting the arguments given on either side of the ST controversy.  But the third point meant that a simple random sample would include very few supporters.  We therefore needed a way to oversample supporters before manual coding, that is, before classifying them as supporters or opponents.  

Further exploration — and a fortuitous coding error — identified a solution.  Using the `spaCy` natural language processing (NLP) suite and its R API `spacyR` *[versions]*, we analyzed the text of each document into individual words ("tokens") and classified each term's part of speech (noun, adjective, etc.).  Specifically, we applied dependency tagging, which enabled us to assemble adjective-noun bigrams (pairs of words), such as "public health" or "regulatory science."  *[filtering large and small docs; comment text and attachments concatenated]*

At this point, [A1] was working on the project by themselves, and had identified the relationship between science and health as the key research question for the project.  [A1] therefore focused on "science or health" bigrams, that is, bigrams where the noun was either "science" or "health."  [A1] identified 405 "science" bigrams and 225 "health"; 215 "science" and 117 "health" bigrams (332 total) occurred in more than one document in the corpus.  15,846 documents contained one or more of the 332 health or health bigrams.  

These 16k documents and 332 bigrams can be represented as a matrix, with documents as rows, bigrams as columns, and each cell containing the number of occurrences of that bigram in that document.  In text mining this is called a "document-term matrix."  

Principal component analysis (PCA) is a statistical technique often used for dimension reduction or data compression, e.g., compressing the 332 columns of bigram data down into, say, 5 or 10 columns *[cite]*.  The resulting columns are referred to as "principal components," and are sometimes interpreted as latent variables measured by the observed variables.  For example, in intelligence testing, a method like PCA is used to construct general intelligence, a latent variable, out of observed test responses.  [A1] hoped that PCA would find a principal component that could be interpreted, at least roughly, as latent support vs. opposition.  However, [A1] miscoded the script, compressing the documents rather than bigrams.  While this error eludes interpretation — the principal components represent continuously-varying latent documents? — the first principal component did appear to provide the desired oversampling:  many of the documents that loaded onto one end of the first principal component supported ST.  

[A1] began to develop a strategy for sampling from across this principal component.  Drawing the most extreme documents from either end resulted in a large number of nearly identical "form letter" comments, most likely collected by advocacy organizations using online forms.  Using phrases that appeared in these comments and simple text matching, [A1] excluded 8,875 from three different forms from the manual coding sample.  After these exclusions, drawing the most extreme 200 documents (including ties) from each end resulted in *[actual numbers, pre-fixing html issue]* 214 from the "bottom" (apparently supportive) and 201 from the "top" (apparently opposed).  Then a simple random sample of 400 comments was drawn from the remaining 6,561 comments (15.8k that contained one or more bigram, less the 8.9k from identified form letters and the 415 from the extremes).  The resulting sample contained 814 comments.^[While manual coding was underway, [A1] prepared draft code for the imputation and analysis steps described below.  This revealed an error in the part-of-speech tagging:  spaCy assumes it is working with plain, pre-cleaned text, but the comment text can contain HTML, especially `<br>` tags.  After fixing this error, the bigram subset was slightly smaller, and some documents used in manual coding were no longer in the bigram subset.  This is why the data used for imputation and analysis is slightly smaller than reported here.]

## Manual coding

At this point, [A1] brought the other collaborators into the project.  Without informing the other authors of the research question, all authors independently coded each comment in the sample as supporting or opposing ST, or too ambiguous to classify.  Krippendorff's alpha was used to assess inter-rater reliability *[https://link.springer.com/article/10.1007/s11135-004-8107-7]*; this statistic ranges from -1 to 1, with 1 indicating perfect agreement across all raters and 0 indicating "no agreement beyond chance" *[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4974794/]*.  After an initial round of independent coding, Krippendorff's alpha was 0.86, with discordance on 74 / 814 comments.  We prepared a spreadsheet that contained only the discordance comments and each author's codes.  After discussing a few examples, each author separately reviewed the discordant comments, and submitted revised coding sheets.  After this one round of revision, Krippendorff's alpha was 0.94, with discordance on 32 / 814 comments.  Inspection of the remaining discordant comments suggested they were generally subject to reasonable disagreement.  For example, we disagreed about whether the following comment (EPA-HQ-OA-2018-0259-10925) should be classified as opposing ST or is too general and therefore irrelevant to ST:  

> We need the Environmental Protection Agency to protect us, and to not loosen restrictions on air and water quality.  Contaminated air and water are terrible for human health.  The EPA should be looking out for people, not fossil fuel companies.  People with respiratory illnesses are especially vulnerable to COVID-19.  As environmental regulations are rolled back, it could be catastrophic for them, as they struggle to breathe already.  I am a registered dietitian, and my work involves helping people to stay as healthy as possible.  This is hard to do on an increasingly  uninhabitable planet.  Our public agencies should be here to protect and work for us.  

Having achieved a very high degree of concordance, we moved on to the next stage of analysis.  


## Imputation

To take advantage of the scale of the ST corpus, we used machine learning to impute or estimate support or opposition across *[all]* 20,787 comments, using the `tidymodels` framework *[software versions]*.  

To assemble the input data, we combined the manually coded comments with the 8.9k form letter comments, treating all of the latter as opposed to ST.  Because ambiguous comments were extremely rare (33/814), we removed them from data, and defined the classification task with a dichotomous "support" or "oppose" variable designated `support`.  Overall, this gave us 9,655 documents, of which 9,499 (98%) opposed ST.  We constructed 3-1 training-testing splits (7,241 and 2,258, respectively) stratified on `support`: 123/7241 = 1.7% supported ST in the training set, and 33/2258 = 1.4% support ST in the testing set.  The training set was further split for 5-fold cross-validation (5792 and 1449 or 5793 and 1448), again stratifying on `support`.  

We defined a model workflow that took the raw text of the comment — comment and attachment text concatenated, as with the NLP tagging — as input.  For processing efficiency, text was truncated after the first 3,000 characters, covering approximately the first 500 words, on the assumption that even long comments would express their support or opposition clearly within the first page or so.  Next, the text was tokenized, stopwords removed *[source]*, and tokens lemmatized.  The resulting token features were hashed to $1024 = 2^10$ features, with signed binary values (0, +1, -1).  Zero variance features were removed.  Finally, because `support` was heavily imbalanced, during training supporting comments were upsampled using simple resampling.  Use of the `tidymodels` framework ensured that this preprocessing pipeline could be applied exactly to both training and testing data, as well as the full corpus, without any risk of data contamination.  

Model tuning was done using 5-fold cross-validation.  Because of class imbalance, balanced accuracy and specificity were chosen as the primary assessment metrics.^[In `tidymodels`, categorical outcomes are handled as factor variables, and by default the "positive" level of the outcome is the first level/value of the factor.  Since `support` takes the values "oppose" and "support," the first level (and so the positive) is opposing ST.  Specificity, true negative rate, thus assesses the model in terms of its ability to identify support for ST.]

We initially explored the use of a random forest model with the `ranger` package, tuning both number of predictors and minimum splittable node size.  However, fitting these models was extremely slow, and cross-validation estimates were lackluster, especially for specificity (roughly 70%).  A logistic lasso model with the `glmnet` package *[version]*, tuning the lasso penalty $\lambda$, was much faster to fit and had much more promising cross-validation estimates, with balanced accuracy around 96-97% and specificity around 93-94% in the best-perfomring models.  

Based on tuning results, we selected a penalty of $\lambda = 0.0385662$ and fit the logistic lasso on the full training data.  This model had a balanced accuracy of 96% and a specificity of 94% on the testing data.  We then used this model to impute dichotomous supporting/opposing codes to the *[full]* corpus of 20.8k comments, without carrying over manual comments.  After imputation, the corpus contained 14,995 opposing comments (72%) and 5,792 supporting comments (28%).  To allow for ambiguous comments, we filtered down to comments where the model was more than 80% confident in its classification (predicted probability of opposition was either greater than 80% or less than 20%).  This filtered subset contained 14,101 opposing comments (74%) and 4,876 supporting comments (26%), for a total of 18,977 (91% of the comments run through imputation).  


## Text analysis


