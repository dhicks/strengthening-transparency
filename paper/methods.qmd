---
format:
  pdf:
    mainfont: "Times New Roman"
    fontsize: "12pt"
    geometry: 
      - "margin = 1in"
    linestretch: 1.5
---

## Corpus assembly

Regulations.gov provides an API (application programming interface) via http, meaning a simple script can be used to automatically retrieve ("scrape") the full contents of a given docket.  [A1] used the API to retrieve all publicly-available comments — including comment metadata, comment text, and any attachments — throughout 2020 and 2021, completing retrieving in June 2021.  

For Strengthening Transparency, Regulations.gov reports 993,464 "comments received" and 22,390 "comments posted to this docket" and publicly available.  *[TODO: email for clarification on relationship between these numbers]*  It's likely that this disparity is due, in part, to separately enumerating some mass comments that were submitted collectively.  For example, comment 11822 (<https://www.regulations.gov/comment/EPA-HQ-OA-2018-0259-11822>) includes 22,786 individual comments collected by Friends of the Earth (likely using an online form) and submitted as large PDFs, one individual comment per page.  

After scraping, our corpus included the text of 22,390 comments and attachments for 20,787 distinct comments.  We refer to this collection of comments and attachments as the *ST corpus*.  By number of attachments, the median document had no attachments and the maximum number of attachments was 17.  

Comments were primarily submitted during two public comment periods, one following the publication of the Notice of Proposed Rulemaking (NPR) on *[date]* and the second in response to the Revised Notice of Proposed Rulemaking (RNPR) on *[date]*.  Comments varied in length dramatically: a few handwritten postcards could not be parsed by OCR and were counted as being 0 words in length, and the longest comment (including the comment text and all attachment text) was 467,917 words long. The median comment was 236 words long, and 95% of comments were 495 words long or less. See figure @fig-date-ecdf. 

*[just use black]*

![(a) Count of submissions, aggregated by week; note the y-axis uses a square root scale. (b) Cumulative frequency of comment lengths, by number of words.](../out/04_date_ecdf.png){#fig-date-ecdf}

## Manual coding

The coding process is summarized in @fig-workflow. 

![Visual summary of the coding workflow. Yellow cells indicate the analysis datasets used in the main text.](workflow.jpg){#fig-workflow}

### Sampling {#sec-sampling}

As discussed in @sec-methods-overview, initial exploration of the comments indicated that we could cleanly distinguish supporters from opponents of ST, but also that a simple random sample would be too unbalanced. We therefore needed a way to oversample supporters before manual coding, that is, before classifying them as supporters or opponents.  

Further exploration — and a serendipitous coding error — identified a solution to the oversampling challenge.  Using the `spaCy` natural language processing (NLP) suite and its R API `spacyR` *[versions]*, we analyzed the text of each document into individual words ("tokens") and classified each term's part of speech (noun, adjective, etc.).  Specifically, we applied dependency tagging, which enabled us to assemble adjective-noun bigrams (pairs of words), such as "public health" or "regulatory science."  *[filtering large and small docs; comment text and attachments concatenated]*

At this point, [A1] was working on the project by themselves, and had identified the relationship between science and health as the key research question for the project.  [A1] therefore focused on "science or health" bigrams, that is, bigrams where the noun was either "science" or "health."  [A1] identified 405 "science" bigrams and 225 "health"; 215 "science" and 117 "health" bigrams (332 total) occurred in more than one document in the corpus.  15,846 documents contained one or more of the 332 health or health bigrams.  

These 16k documents and 332 bigrams can be represented as a matrix, with documents as rows, bigrams as columns, and each cell containing the number of occurrences of that bigram in that document.  In text mining this is called a "document-term matrix."  

Principal component analysis (PCA) is a statistical technique often used for dimension reduction or data compression, e.g., compressing the 332 columns of bigram data down into, say, 5 or 10 columns *[cite]*.  The resulting columns are referred to as "principal components," and are sometimes interpreted as latent variables measured by the observed variables.  For example, in intelligence testing, a method like PCA is used to construct general intelligence, a latent variable, out of observed test responses.  [A1] hoped that PCA would find a principal component that could be interpreted, at least roughly, as latent support vs. opposition.  However, [A1] miscoded the script, compressing the documents rather than bigrams.  While this error eludes interpretation — the principal components represent continuously-varying latent documents? — the first principal component did appear to provide the desired oversampling:  many of the documents that loaded onto one end of the first principal component supported ST.  

[A1] began to develop a strategy for sampling from across this principal component.  Drawing the most extreme documents from either end resulted in a large number of nearly identical "form letter" comments, most likely collected by advocacy organizations using online forms.  Using phrases that appeared in these comments and simple text matching, [A1] excluded 8,875 comments matching three different forms from the manual coding sample.  After these exclusions, drawing the most extreme 200 documents (including ties) from each end resulted in *[actual numbers, pre-fixing html issue]* 214 from the "bottom" (apparently supportive) and 201 from the "top" (apparently opposed).  Then a simple random sample of 400 comments was drawn from the remaining 6,561 comments (15.8k that contained one or more bigram, less the 8.9k from identified form letters and the 415 from the extremes).  The resulting sample contained 814 comments.^[While manual coding was underway, [A1] prepared draft code for the imputation and analysis steps described below.  This revealed an error in the part-of-speech tagging:  spaCy assumes it is working with plain, pre-cleaned text, but the comment text can contain HTML, especially `<br>` tags.  After fixing this error, the bigram subset was slightly smaller, and some documents used in manual coding were no longer in the bigram subset.  This is why the data used for imputation and analysis is slightly smaller than reported here.]

### Manual coding

At this point, [A1] brought the other collaborators into the project.  Without informing the other authors of the research question, all authors independently coded each comment in the sample as supporting or opposing ST, or too ambiguous to classify.^[Among qualitative social scientists, this is referred to as "deductive coding." "Inductive" or "open" coding, by contrast, does not start with pre-defined codes, and is about developing a novel coding scheme, in a rather Baconian sense of induction.]  Krippendorff's alpha was used to assess inter-rater reliability *[https://link.springer.com/article/10.1007/s11135-004-8107-7]*; this statistic ranges from -1 to 1, with 1 indicating perfect agreement across all raters and 0 indicating "no agreement beyond chance" *[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4974794/]*.  After an initial round of independent coding, Krippendorff's alpha was 0.86, with discordance on 74 / 814 comments.  We prepared a spreadsheet that contained only the discordance comments and each author's codes.  After discussing a few examples, each author separately reviewed the discordant comments, and submitted revised coding sheets.  After this one round of revision, Krippendorff's alpha was 0.94, with discordance on 32 / 814 comments.  Inspection of the remaining discordant comments suggested they were generally subject to reasonable disagreement.  For example, we disagreed about whether the following comment (EPA-HQ-OA-2018-0259-10925) should be classified as opposing ST or is too general and therefore irrelevant to ST:  

> We need the Environmental Protection Agency to protect us, and to not loosen restrictions on air and water quality.  Contaminated air and water are terrible for human health.  The EPA should be looking out for people, not fossil fuel companies.  People with respiratory illnesses are especially vulnerable to COVID-19.  As environmental regulations are rolled back, it could be catastrophic for them, as they struggle to breathe already.  I am a registered dietitian, and my work involves helping people to stay as healthy as possible.  This is hard to do on an increasingly  uninhabitable planet.  Our public agencies should be here to protect and work for us.  

Having achieved a very high degree of concordance, we moved on to the next stage of analysis.  


### Imputation

To take advantage of the scale of the ST corpus, we used a machine learning model to impute or estimate support or opposition across *[all]* 20,787 comments, using the `tidymodels` framework *[software versions]* [compare this use of supervised machine learning to the approach sketched by @VaesenPotentialSupervisedMachine2022].  

To assemble the input data, we combined the manually coded comments with the 8.9k form letter comments, treating all of the latter as opposed to ST.  Because ambiguous comments were extremely rare (33/814), we removed them from data, and defined the classification task with a dichotomous "support" or "oppose" variable designated `support`.  Overall, this gave us 9,655 documents, of which 9,499 (98%) opposed ST.  We constructed 3-1 training-testing splits (7,241 and 2,258, respectively) stratified on `support`: 123/7241 = 1.7% supported ST in the training set, and 33/2258 = 1.4% support ST in the testing set.  The training set was further split for 5-fold cross-validation (5792 and 1449 or 5793 and 1448), again stratifying on `support`.  

We defined a model workflow that took the raw text of the comment — comment and attachment text concatenated, as with the NLP tagging — as input.  For processing efficiency, text was truncated after the first 3,000 characters, covering approximately the first 500 words, on the assumption that even long comments would express their support or opposition clearly within the first page or so.  Next, the text was tokenized, stopwords removed *[source]*, and tokens lemmatized.  The resulting token features were hashed to $1024 = 2^10^$ features, with signed binary values (0, +1, -1).  Zero variance features were removed.  Finally, because `support` was heavily imbalanced, during training supporting comments were upsampled using simple resampling.  Use of the `tidymodels` framework ensured that this preprocessing pipeline could be applied exactly to both training and testing data, as well as the full corpus, without any risk of data contamination.  

Model tuning was done using 5-fold cross-validation.  Because of class imbalance, balanced accuracy and specificity were chosen as the primary assessment metrics.^[In `tidymodels`, categorical outcomes are handled as factor variables, and by default the "positive" level of the outcome is the first level/value of the factor.  Since `support` takes the values "oppose" and "support," the first level (and so the positive) is opposing ST.  Specificity, true negative rate, thus assesses the model in terms of its ability to identify support for ST.]

We initially explored the use of a random forest model with the `ranger` package, tuning both number of predictors and minimum splittable node size.  However, fitting these models was extremely slow, and cross-validation estimates were lackluster, especially for specificity (roughly 70%).  A logistic lasso model with the `glmnet` package *[version]*, tuning the lasso penalty $\lambda$, was much faster to fit and had much more promising cross-validation estimates, with balanced accuracy around 96-97% and specificity around 93-94% in the best-perfomring models.  

Based on tuning results, we selected a penalty of $\lambda = 0.0385662$ and fit the logistic lasso on the full training data.  This model had a balanced accuracy of 96% and a specificity of 94% on the testing data.  We then used this model to impute dichotomous supporting/opposing codes to the *[full]* corpus of 20.8k comments, without carrying over manual comments.  After imputation, the corpus contained 14,995 opposing comments (72%) and 5,792 supporting comments (28%).  To allow for ambiguous comments, we filtered down to comments where the model was more than 80% confident in its classification (predicted probability of opposition was either greater than 80% or less than 20%).  This filtered subset contained 14,101 opposing comments (74%) and 4,876 supporting comments (26%), for a total of 18,977 (91% of the comments run through imputation).  

*[The distribution of document lengths, in words (spaCy-annotated tokens), after coding is shown in fig @fig-len. 11_len.png]*


## Keyword synonyms and exclusions {#sec-embeddings}

Keyword detection is vulnerable to both false negatives (ignoring synonyms or closely-related concepts that do not strictly match chosen keywords) and false positives (counting unrelated concepts that do match chosen keywords).  For example, use of the rule's title, "Strengthening Transparency in Regulatory Science," should not count as evoking or referring to science.  

To analyze the corpus for these possibilities, we implemented a lightweight variation on word embeddings.  Word embeddings attempt to represent words (or whatever unit of text) as vectors in an abstract space such that two vectors point in the same direction (more formally: have high cosine similarity) insofar as the corresponding words are semantically similar.  While word embeddings are usually created using neural network techniques, *[<https://proceedings.neurips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html>]* found that a less computationally-intensive method (partial singular value decomposition [SVD] of a pointwise mutual information matrix) produced similar results.  We implemented this method in R, following the example of *[Silge blogpost]*.  

Specifically, we first filtered the spaCy annotated unigrams (from 20,787 comments) to those that were tagged as adjectives, nouns, verbs, and adverbs, since we judged these to be the most likely to contain keyword matches.  Tokens were constructed as a combination of a word lemma (standardized word form) and part of speech tag, such as `scientifically_ADV`.  This resulted in 78,744 distinct tokens.  Marginal occurrence probabilities (across the entire corpus annotated with spaCy) were calculated for each token.  Two tokens were determined to co-occur if they occurred in the same sentence (as identified by spaCy) in the same comment/attachment.  Co-occurrences were filtered at 20; that is, if a pair of tokens had 20 or fewer co-occurrences, this was treated as 0 co-occurrences.  After filtering, 8,364 tokens had at least one non-zero occurrence.  Co-occurrence / joint probabilities were calculated, and these were combined with marginal probabilities to calculate pointwise mutual information (pmi): 

$$ pmi(x, y) = \log \frac{p(x, y)}{p(x) p(y) }$$

We then calculated the partial SVD of this pmi matrix using the `irlba` package *[version]*, selecting 256 dimensions.  This resulted in a 8,364 $\times$ 256 embedding matrix, with rows representing tokens and columns representing dimensions of the embedding space.  

Given a keyword search string, such as `scien`, we first searched the embedding vocabulary (8,364 tokens) for matches.  For each match, identifying similar tokens was simply a matter of matrix multiplication and sorting.  The sorted results were filtered to have the same part of speech as the match, and then the top 5 results were displayed for each match.  We perused the resulting list to identify potential synonyms and matches that needed to be excluded.  Some additional, multiword false positive matches were identified based on our familiarity with the corpus, such as "Environmental Protection Agency" or the title of ST.  


## Text analysis

Text analysis was conducted as reported in the main text, with code written in R *[version]* and making extensive use of the `tidyverse` suite *[version]*. 


## Inferential statistics and inductive scope

While the ST corpus can be considered a sample of US public opinion on ST, almost all comments come from a relatively highly engaged and haphazard subset:  people on email lists for environmental advocacy organizations, regulated industries, state regulators, readers of anti-environmentalist blogs.  It therefore cannot be considered a representative sample of anyone except "the kinds of people who were likely to have submitted comments on ST."  And for this population the ST corpus is, as it were, a nearly complete sample:  insofar as someone was likely to have submitted comments on ST, we probably have a comment from them in the ST corpus.  

Thus, we believe that so-called inferential statistics — things like p-values from statistical hypothesis tests and confidence intervals — would be misleading, as they would incorrectly indicate that we can formally generalize from the ST corpus to some more-or-less well-defined larger population.  We therefore did not calculate, and do not report here, any such statistics.  


## Data and code availability

The full data collection and analysis code is available at *[github]*.  To facilitate reproduction and reuse, *[separate data repo]*  

- raw ST scrape results? 
- 03 raw text and annotation files
- 06 manual coding
- 08 fitted model and imputed dataset




